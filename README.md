# ğŸ” Serper AI Search Assistant

An AI-powered search assistant that combines **real-time web search** with **LLM-based summarization** to deliver concise, human-friendly answers to your queries â€” complete with source links.

## âœ¨ Features

- **Web Search via Serper API** â€” fetches the top Google results for any query
- **LLM-Powered Answers via Ollama** â€” summarizes search results into natural, conversational responses
- **Smart Query Rewriting** â€” automatically expands a single question into multiple sub-queries for richer context
- **LRU Response Cache** â€” caches up to 100 recent answers for instant repeat lookups
- **Markdown Rendering** â€” assistant responses are rendered with full Markdown & GFM support
- **Modern Chat UI** â€” clean React interface with typing indicators, auto-scroll, and an empty-state landing

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        POST /query        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚   Backend    â”‚
â”‚  (React +    â”‚  â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  (Express)   â”‚
â”‚   Vite)      â”‚       JSON response       â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â–¼             â–¼             â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚  Serper  â”‚ â”‚  Ollama  â”‚ â”‚ LRU Cache  â”‚
                              â”‚  API     â”‚ â”‚  LLM     â”‚ â”‚ (in-mem)   â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‚ Project Structure

```
serper/
â”œâ”€â”€ Backend/
â”‚   â”œâ”€â”€ server.js       # Express API â€” search, LLM, caching, query endpoint
â”‚   â”œâ”€â”€ helper.js       # Query rewriting via Ollama
â”‚   â”œâ”€â”€ .env            # API keys & config (SERPER_API_KEY, OLLAMA_URL, MODEL)
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ Frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx             # Main chat application
â”‚   â”‚   â”œâ”€â”€ App.css             # Styling
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ Header.jsx          # App header / branding
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInput.jsx       # Query input bar
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatMessage.jsx     # Message bubble (user & assistant)
â”‚   â”‚   â”‚   â”œâ”€â”€ TypingIndicator.jsx # Loading animation
â”‚   â”‚   â”‚   â””â”€â”€ EmptyState.jsx      # Landing state when no messages
â”‚   â”‚   â”œâ”€â”€ main.jsx            # React entry point
â”‚   â”‚   â””â”€â”€ index.css           # Global styles
â”‚   â”œâ”€â”€ .env            # Frontend config (BACKEND_URL)
â”‚   â”œâ”€â”€ vite.config.js
â”‚   â””â”€â”€ package.json
â””â”€â”€ README.md
```

## ğŸš€ Getting Started

### Prerequisites

| Tool               | Purpose                                                  |
| ------------------ | -------------------------------------------------------- |
| **Node.js** (v18+) | Runtime for both frontend & backend                      |
| **npm**            | Package manager                                          |
| **Ollama**         | Local LLM inference ([ollama.com](https://ollama.com))   |
| **Serper API Key** | Google search results ([serper.dev](https://serper.dev)) |

### 1. Clone the Repository

```bash
git clone <repo-url>
cd serper
```

### 2. Set Up the Backend

```bash
cd Backend
npm install
```

Create / edit the `.env` file:

```env
SERPER_API_KEY=your_serper_api_key
OLLAMA_URL=http://localhost:11434
MODEL=your_model_name
```

Start the server:

```bash
node server.js
```

The backend will start on **http://localhost:8007**.

### 3. Set Up the Frontend

```bash
cd Frontend
npm install
```

Start the dev server:

```bash
npm run dev
```

The frontend will start on **http://localhost:5173** (default Vite port).

### 4. Start Ollama

Make sure Ollama is running and the model specified in `Backend/.env` is available:

```bash
ollama serve
ollama pull <model_name>
```

## ğŸ”§ API Reference

### `POST /query`

Send a search query and receive an AI-summarized answer.

**Request Body:**

```json
{
  "query": "What is the latest iPhone price?"
}
```

**Response:**

```json
{
  "response": "The latest iPhone 16 starts at $799 for the base model..."
}
```

**Error Codes:**

| Status | Reason                                              |
| ------ | --------------------------------------------------- |
| `400`  | Missing or invalid query / query exceeds 2000 chars |
| `500`  | Internal server error (search or LLM failure)       |

## ğŸ› ï¸ Tech Stack

| Layer    | Technology                                   |
| -------- | -------------------------------------------- |
| Frontend | React 19, Vite 7, react-markdown, remark-gfm |
| Backend  | Express 5, Axios, dotenv                     |
| Search   | Serper API (Google Search)                   |
| LLM      | Ollama (local inference)                     |

## ğŸ“„ License

ISC
# Web-SearchBot
